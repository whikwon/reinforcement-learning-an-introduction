{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld\n",
    "\n",
    "1. 문제: 4x4 격자로 이루어진 판 위에서 규칙에 따라 이동할 때 위치(state) 별 value function 값을 구해보자.\n",
    "2. 조건\n",
    "    - state: 말의 위치\n",
    "    - action: 상,하,좌,우\n",
    "    - reward: 이동(-1), 좌측 상단, 우측 하단에서 이동(0, terminal)\n",
    "3. 학습 목표\n",
    "    - policy evaluation \n",
    "        - Bellman equation\n",
    "            - episode 별로 value function 확인\n",
    "            - 무한히 돌린 경우 value function 수렴 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridworld\n",
    "- 5x5 격자 내 각각의 위치(state)에서 1번의 action만 수행한다. \n",
    "- 따로 초기 값이 주어지지 않는다. \n",
    "- 특정 state에서 action이 행해졌을 때 MDP가 주어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_SIZE = 4\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "L_CORNER = [0, 0]\n",
    "R_CORNET = [3, 3]\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, shape=[4, 4]):        \n",
    "        self.nA = 4\n",
    "        self.nS = np.prod(shape)\n",
    "        \n",
    "        grid = np.arange(WORLD_SIZE ** 2).reshape(shape)\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "        \n",
    "        P = defaultdict(lambda: [[] for i in range(self.nA)])\n",
    "        while not it.finished: \n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "                    \n",
    "            is_done = lambda s: s == self.nS - 1 or s == 0\n",
    "            MAX_Y = shape[0]\n",
    "            MAX_X = shape[1]\n",
    "            \n",
    "            if is_done(s):\n",
    "                P[s][UP] = [1.0, s, 0, True]\n",
    "                P[s][DOWN] = [1.0, s, 0, True]\n",
    "                P[s][LEFT] = [1.0, s, 0, True]\n",
    "                P[s][RIGHT] = [1.0, s, 0, True]            \n",
    "            else:\n",
    "                s_up = s - MAX_Y if y != 0 else s\n",
    "                s_down = s + MAX_Y if y != MAX_Y-1 else s\n",
    "                s_left = s - 1 if x != 0 else s\n",
    "                s_right = s + 1 if x != MAX_X-1 else s\n",
    "                \n",
    "                P[s][UP] = [1.0, s_up, -1, False]\n",
    "                P[s][DOWN] = [1.0, s_down, -1, False]\n",
    "                P[s][LEFT] = [1.0, s_left, -1, False]\n",
    "                P[s][RIGHT] = [1.0, s_right, -1, False]\n",
    "            it.iternext()\n",
    "                \n",
    "        self.P = P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random policy\n",
    "상, 하, 좌, 우 같은 확률로 주어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.array([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative policy evaluation\n",
    "bellman equation을 iterative하게 update해서 구하는 식으로 변형해서 value function을 구할 수 있다. 아래 식에서 $v_{\\pi}$가 존재하는 경우 $k \\rightarrow \\infty$의 조건에서 $v_k$는 $v_{\\pi}$로 수렴한다. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{k+1}(s) &\\doteq \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_k(S_{t+1} \\lvert S_t = s] \\\\\n",
    "&= \\displaystyle \\sum_a \\pi(a \\lvert s) \\sum_{s', r} p(s', r \\lvert s, a) \\big[ r + \\gamma v_k(s') \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "![itertive_policy_evaluation](../images/itertive_policy_evaluation.png)\n",
    "<center> 출처: Reinforcement Learning : An Introduction. Richard S. Sutton and Andrew G. Barto. 2017 (pp. 61) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드에서 `num_episode`($k$)에 따라 value function 값이 어떻게 변하는지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_iteration(env, k, policy, discount_factor=1.0):\n",
    "    V = np.zeros(env.nS)\n",
    "    for i_episode in range(k):\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v += action_prob * prob * (reward + discount_factor * old_V[next_state])\n",
    "            V[s] = v            \n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)\n",
    "\n",
    "def check_value_iteration(policy, k_list=[0, 1, 2, 3, 10]):\n",
    "    for k in k_list:\n",
    "        print('=' * 50)\n",
    "        print('k = {}'.format(k))\n",
    "        value = policy_evaluation(env, k, policy)\n",
    "        print(value)\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "k = 0\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "==================================================\n",
      "k = 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "==================================================\n",
      "k = 2\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "==================================================\n",
      "k = 3\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "==================================================\n",
      "k = 10\n",
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "check_value_iteration(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterative policy evaluation으로 주어진 policy에서의 value function을 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(env, policy, discount_factor=1.0, theta=1e-5):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v += action_prob * prob * (reward + discount_factor*old_V[next_state])\n",
    "            V[s] = v\n",
    "            delta = max(delta, np.abs(old_V[s] - v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99989315 -19.99984167 -21.99982282]\n",
      " [-13.99989315 -17.99986052 -19.99984273 -19.99984167]\n",
      " [-19.99984167 -19.99984273 -17.99986052 -13.99989315]\n",
      " [-21.99982282 -19.99984167 -13.99989315   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "value = iterative_policy_evaluation(env, policy)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman optimality equation\n",
    "bellman optimality equation을 통해 주어진 policy와 상관없이 optimal value function을 구할 수 있다.\n",
    "\n",
    "$$\\begin{align} \n",
    "v_{\\ast}(s) &= \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_{\\ast}(S_{t+1}) \\lvert S_t = s, A_t = a] \\\\\n",
    "&= \\max_a \\displaystyle \\sum_{s', r} p(s', r \\lvert s, a) \\big [r+\\gamma v_{\\ast}(s') \\big] \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimality_equation(env, policy, discount_factor=1.0, theta=1e-5):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v_candidate = []\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v_candidate.append(prob * (reward + discount_factor*old_V[next_state]))\n",
    "            V[s] = max(v_candidate)\n",
    "            delta = max(delta, np.abs(old_V[s] - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "optimal_value = bellman_optimality_equation(env, policy)\n",
    "print(optimal_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
