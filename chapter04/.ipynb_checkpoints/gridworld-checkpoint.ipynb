{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld\n",
    "\n",
    "1. 문제: 4x4 격자로 이루어진 판 위에서 규칙에 따라 이동할 때 위치(state) 별로 value function 값을 구해보자.\n",
    "2. 조건\n",
    "    - state: 말의 위치\n",
    "    - action: 상,하,좌,우 (grid를 벗어나지 못한다.) \n",
    "    - reward: 좌측 상단, 우측 하단에 있을 경우 reward +0(terminal), 나머지 경우 움직임마다 reward -1\n",
    "3. 학습 목표\n",
    "    - policy evaluation \n",
    "        - Bellman equation\n",
    "            - episode 별로 value function 확인\n",
    "            - 무한히 돌린 경우 value function 수렴 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORLD_SIZE = 4\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "L_CORNER = [0, 0]\n",
    "R_CORNET = [3, 3]\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, shape=[4, 4]):        \n",
    "        self.nA = 4\n",
    "        self.nS = np.prod(shape)\n",
    "        \n",
    "        grid = np.arange(WORLD_SIZE**2).reshape(shape)\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "        \n",
    "        P = defaultdict(lambda: [[] for i in range(self.nA)])\n",
    "        while not it.finished: \n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "                    \n",
    "            is_done = lambda s: s == self.nS - 1 or s == 0\n",
    "            MAX_Y = shape[0]\n",
    "            MAX_X = shape[1]\n",
    "            \n",
    "            if is_done(s):\n",
    "                P[s][UP] = [1.0, s, 0, True]\n",
    "                P[s][DOWN] = [1.0, s, 0, True]\n",
    "                P[s][LEFT] = [1.0, s, 0, True]\n",
    "                P[s][RIGHT] = [1.0, s, 0, True]            \n",
    "            else:\n",
    "                s_up = s - MAX_Y if y != 0 else s\n",
    "                s_down = s + MAX_Y if y != MAX_Y-1 else s\n",
    "                s_left = s - 1 if x != 0 else s\n",
    "                s_right = s + 1 if x != MAX_X-1 else s\n",
    "                \n",
    "                P[s][UP] = [1.0, s_up, -1, False]\n",
    "                P[s][DOWN] = [1.0, s_down, -1, False]\n",
    "                P[s][LEFT] = [1.0, s_left, -1, False]\n",
    "                P[s][RIGHT] = [1.0, s_right, -1, False]\n",
    "            it.iternext()\n",
    "                \n",
    "        self.P = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.array([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative policy evaluation\n",
    "아래 식은 policy가 주어졌을 때 value function의 값을 구하는 **Bellman equation** 이다. random policy를 집어넣은 뒤에 문제를 풀어보자.\n",
    "\n",
    "$$v_{\\pi}(s) = \\displaystyle \\sum_{a} \\pi(a \\vert s) \\sum_{s', r} p(s',r \\lvert s, a) \\big [r+\\gamma v_{\\pi}(s') \\big], \\text{ for all } s \\in S : \\text{ Bellman equation for }v_{\\pi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, num_episodes, policy, discount_factor=1.0):\n",
    "    V = np.zeros(env.nS)\n",
    "    for i_episode in range(num_episodes):\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v += action_prob * prob * (reward + discount_factor*old_V[next_state])\n",
    "            V[s] = v\n",
    "            \n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_value_iteration(policy, episodes_list=[0, 1, 2, 3, 10]):\n",
    "    for k in episodes_list:\n",
    "        print('=' * 50)\n",
    "        print('k={}'.format(k))\n",
    "        value = policy_evaluation(env, k, policy)\n",
    "        print(value)\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "k=0\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "==================================================\n",
      "k=1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "==================================================\n",
      "k=2\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "==================================================\n",
      "k=3\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "==================================================\n",
      "k=10\n",
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "check_value_iteration(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 무한히(*는 불가능하므로 엄청 true value와 거의 같아질 때까지*) 돌렸을 때 true value function 값으로 수렴하는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, discount_factor=1.0, theta=1e-5):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v += action_prob * prob * (reward + discount_factor*old_V[next_state])\n",
    "            V[s] = v\n",
    "            delta = max(delta, np.abs(old_V[s] - v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "value = policy_evaluation(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99989315 -19.99984167 -21.99982282]\n",
      " [-13.99989315 -17.99986052 -19.99984273 -19.99984167]\n",
      " [-19.99984167 -19.99984273 -17.99986052 -13.99989315]\n",
      " [-21.99982282 -19.99984167 -13.99989315   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
