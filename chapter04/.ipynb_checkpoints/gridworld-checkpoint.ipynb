{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld\n",
    "\n",
    "1. 문제: 4x4 격자로 이루어진 판 위에서 규칙에 따라 이동할 때 위치(state) 별로 value function 값을 구해보자.\n",
    "2. 조건\n",
    "    - state: 말의 위치\n",
    "    - action: 상,하,좌,우 (grid를 벗어나지 못한다.) \n",
    "    - reward: 좌측 상단, 우측 하단에 있을 경우 reward +0(terminal), 나머지 경우 움직임마다 reward -1\n",
    "3. 학습 목표\n",
    "    - policy evaluation \n",
    "        - Bellman equation\n",
    "            - episode 별로 value function 확인\n",
    "            - 무한히 돌린 경우 value function 수렴 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORLD_SIZE = 4\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "L_CORNER = [0, 0]\n",
    "R_CORNET = [3, 3]\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, shape=[4, 4]):        \n",
    "        self.nA = 4\n",
    "        self.nS = np.prod(shape)\n",
    "        \n",
    "        grid = np.arange(WORLD_SIZE**2).reshape(shape)\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "        \n",
    "        P = defaultdict(lambda: [[] for i in range(self.nA)])\n",
    "        while not it.finished: \n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "                    \n",
    "            is_done = lambda s: s == self.nS - 1 or s == 0\n",
    "            MAX_Y = shape[0]\n",
    "            MAX_X = shape[1]\n",
    "            \n",
    "            if is_done(s):\n",
    "                P[s][UP] = [1.0, s, 0, True]\n",
    "                P[s][DOWN] = [1.0, s, 0, True]\n",
    "                P[s][LEFT] = [1.0, s, 0, True]\n",
    "                P[s][RIGHT] = [1.0, s, 0, True]            \n",
    "            else:\n",
    "                s_up = s - MAX_Y if y != 0 else s\n",
    "                s_down = s + MAX_Y if y != MAX_Y-1 else s\n",
    "                s_left = s - 1 if x != 0 else s\n",
    "                s_right = s + 1 if x != MAX_X-1 else s\n",
    "                \n",
    "                P[s][UP] = [1.0, s_up, -1, False]\n",
    "                P[s][DOWN] = [1.0, s_down, -1, False]\n",
    "                P[s][LEFT] = [1.0, s_left, -1, False]\n",
    "                P[s][RIGHT] = [1.0, s_right, -1, False]\n",
    "            it.iternext()\n",
    "                \n",
    "        self.P = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.array([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative policy evaluation\n",
    "아래 식은 policy가 주어졌을 때 value function의 값을 구하는 **Bellman equation** 이다. random policy를 집어넣은 뒤에 문제를 풀어보자. \n",
    "\n",
    "$$v_{\\pi}(s) = \\displaystyle \\sum_{a} \\pi(a \\vert s) \\sum_{s', r} p(s',r \\lvert s, a) \\big [r+\\gamma v_{\\pi}(s') \\big], \\text{ for all } s \\in S : \\text{ Bellman equation for }v_{\\pi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, num_episodes, policy, discount_factor=1.0):\n",
    "    V = np.zeros(env.nS)\n",
    "    for i_episode in range(num_episodes):\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v += action_prob * prob * (reward + discount_factor*old_V[next_state])\n",
    "            V[s] = v\n",
    "            \n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_value_iteration(policy, episodes_list=[0, 1, 2, 3, 10]):\n",
    "    for k in episodes_list:\n",
    "        print('=' * 50)\n",
    "        print('k={}'.format(k))\n",
    "        value = policy_evaluation(env, k, policy)\n",
    "        print(value)\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "k=0\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "==================================================\n",
      "k=1\n",
      "[[ 0.   -0.67 -0.67 -0.67]\n",
      " [-0.67 -0.67 -0.67 -0.67]\n",
      " [-0.67 -0.67 -0.67 -0.67]\n",
      " [-0.67 -0.67 -0.67  0.  ]]\n",
      "==================================================\n",
      "k=2\n",
      "[[ 0.     -1.0519 -1.1189 -1.1189]\n",
      " [-0.7839 -1.1189 -1.1189 -1.1189]\n",
      " [-1.1189 -1.1189 -1.1189 -1.0854]\n",
      " [-1.1189 -1.1189 -1.1055  0.    ]]\n",
      "==================================================\n",
      "k=3\n",
      "[[ 0.       -1.274273 -1.412963 -1.419663]\n",
      " [-0.826713 -1.352663 -1.419663 -1.417988]\n",
      " [-1.252163 -1.419663 -1.418323 -1.363048]\n",
      " [-1.419663 -1.419395 -1.396615  0.      ]]\n",
      "==================================================\n",
      "k=10\n",
      "[[ 0.         -1.57692275 -1.91193624 -1.97846151]\n",
      " [-0.85626971 -1.6676335  -1.9215035  -1.97461284]\n",
      " [-1.3492236  -1.76624095 -1.93305901 -1.87879063]\n",
      " [-1.62450937 -1.84247471 -1.90915563  0.        ]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "check_value_iteration(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 무한히(*는 불가능하므로 엄청 true value와 거의 같아질 때까지*) 돌렸을 때 true value function 값으로 수렴하는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, discount_factor=1.0, theta=1e-5):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v += action_prob * prob * (reward + discount_factor*old_V[next_state])\n",
    "            V[s] = v\n",
    "            delta = max(delta, np.abs(old_V[s] - v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman optimality equation으로 optimal value는 얼마로 수렴하는지 확인해보자. policy evaluation으로 $v_{\\pi}$를 구하는 목적은 결국 $v_{\\ast}$를 알기 위함이다. \n",
    "\n",
    "근데, reward를 누적하는 방법보다 iterative 하게 계산하는게 효율적이므로 $v$를 기억해서 진행하는 방식을 택하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bellman_optimality_equation(env, policy, discount_factor=1.0, theta=1e-5):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        old_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v_candidate = []\n",
    "            for a, action_prob in enumerate(policy):\n",
    "                prob, next_state, reward, done = env.P[s][a]\n",
    "                v_candidate.append(prob * (reward + discount_factor*old_V[next_state]))\n",
    "            V[s] = max(v_candidate)\n",
    "            delta = max(delta, np.abs(old_V[s] - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V.reshape(WORLD_SIZE, WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "value = policy_evaluation(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.58481278 -1.93131987 -2.00680627]\n",
      " [-0.85668839 -1.67561307 -1.94089286 -2.00277992]\n",
      " [-1.35014652 -1.77441291 -1.95236258 -1.90471694]\n",
      " [-1.62598967 -1.85090293 -1.92765223  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "value = bellman_optimality_equation(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
