{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baird\n",
    "1. 문제:\n",
    "2. 조건\n",
    "    - state: 현재 node 위치\n",
    "        - feature vector($\\textbf{x}$): 현재 state를 $w_1,...w_8$을 이용해서 vector로 나타낸다.\n",
    "    - action\n",
    "        - SOLID: 무조건 아래 쪽 state(7번)로 이동한다.\n",
    "        - DASH: 무작위로 위 쪽 state(1~6번)로 이동한다. \n",
    "    - reward: 모든 time step에서 0이다.\n",
    "3. 학습 목표\n",
    "    - Semi-gradient Off-policy TD\n",
    "    - Semi-gradient DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = np.arange(0, 7)\n",
    "LOWER_STATE = 6\n",
    "FEATURE_SIZE = 8\n",
    "DASH = 0\n",
    "SOLID = 1\n",
    "BEHAVIOR_SOLID_PROBABILITY = 1.0 / 7\n",
    "\n",
    "def target_policy():\n",
    "    return SOLID\n",
    "\n",
    "def behavior_policy():\n",
    "    if np.random.binomial(1, BEHAVIOR_SOLID_PROBABILITY) == 1:\n",
    "        return SOLID\n",
    "    return DASH\n",
    "\n",
    "class Baird:\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        if action == SOLID:\n",
    "            self.state = LOWER_STATE\n",
    "        else:\n",
    "            self.state = np.random.choice(STATES[: LOWER_STATE])\n",
    "        return self.state, reward            \n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "    def __init__(self, step_size):\n",
    "        self.step_size = step_size\n",
    "        self.weights = np.array([1, 1, 1, 1, 1, 1, 10, 1], dtype=np.float32)\n",
    "        self.features = np.zeros([len(STATES), FEATURE_SIZE])\n",
    "        for i in range(LOWER_STATE):\n",
    "            self.features[i, i] = 2\n",
    "            self.features[i, 7] = 1\n",
    "        self.features[LOWER_STATE, 6] = 1\n",
    "        self.features[LOWER_STATE, 7] = 2        \n",
    "        \n",
    "    def value(self, state):\n",
    "        return self.features[state]@self.weights\n",
    "    \n",
    "    def learn(self, state, delta, rho):\n",
    "        self.weights += self.features[state] * self.step_size * rho * delta        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "step_size = 0.01\n",
    "env = Baird()\n",
    "value_fn = ValueFunction(step_size)\n",
    "policy_fn = behavior_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi Gradient Off-policy TD(0)\n",
    "importance sampling, value function approximation을 활용해서 off-policy 학습을 진행한다. \n",
    "\n",
    "식은 아래 2개 식을 참조해서 구현했고 사용한 value function approximator는 linear function으로 $v(S_t, \\textbf{w}) = \\textbf{x} \\cdot \\textbf{w}$, (*$\\textbf{x}$는 feature vector이다.*)\n",
    "\n",
    "$\\delta_t = R_{t+1} + \\gamma \\hat v(S_{t+1}, \\textbf{w}_t) - \\hat v(S_t, \\textbf{w})$ <br> \n",
    "$w_{t+1} = w_t + \\alpha \\rho_t \\delta_t \\nabla \\hat v (S_t, w_t)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semi_gradient_off_policy_TD(env, num_steps, value_fn, behavior_policy_fn, \n",
    "                                discount_factor=0.95):\n",
    "    state = env.reset()\n",
    "    for i in range(num_steps):\n",
    "        action = behavior_policy_fn()    \n",
    "        reward, next_state = env.step(action)\n",
    "        delta = reward + discount_factor * value_fn.value(next_state) - value_fn.value(state)\n",
    "        # target policy에 대해서 일반화해서 구현 vs 이 문제에 대해서만 쉽게 구현\n",
    "        if action == DASH:\n",
    "            rho = 0.0\n",
    "        else:\n",
    "            rho = 1.0 / BEHAVIOR_SOLID_PROBABILITY\n",
    "        value_fn.learn(state, delta, rho)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_gradient_off_policy_TD(env, num_steps, value_fn, policy_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 45.0723381 ,   1.        ,   1.        ,   1.        ,\n",
       "         1.        ,   1.        ,  10.        ,  23.03616714], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_fn.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
