{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld\n",
    "\n",
    "1. 문제: 5x5 격자로 이루어진 판 위 여러 규칙 내에서 이동할 때 위치(state) 별로 value function 값을 구해보자.\n",
    "2. 조건\n",
    "    - state: 말의 위치\n",
    "    - action: 상,하,좌,우 \n",
    "    - reward: 벽에 부딪혔을 때(-1), 이동(0), 목적지 도달(A': +10, B': +5)\n",
    "3. 학습 목표\n",
    "    - Policy evaluation\n",
    "        - Bellman equation\n",
    "        - Bellman optimality equation 구현을 통해 value function 값을 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORLD_SIZE = 5\n",
    "A_POS = (0, 1)\n",
    "A_PRIME_POS = [4, 1]\n",
    "B_POS = (0, 3)\n",
    "B_PRIME_POS = [2, 4]\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3 \n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.nA = 4\n",
    "        self.nS = WORLD_SIZE * WORLD_SIZE\n",
    "\n",
    "        P = defaultdict(lambda: [[] for i in range(WORLD_SIZE - 1)])\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                s = (i, j)\n",
    "                if i == 0:\n",
    "                    P[s][UP] = [1.0, [i, j], -1.0, False]\n",
    "                else:\n",
    "                    P[s][UP] = [1.0, [i-1, j], 0.0, False]\n",
    "                if i == WORLD_SIZE - 1:\n",
    "                    P[s][DOWN] = [1.0, [i, j], -1.0, False]\n",
    "                else:\n",
    "                    P[s][DOWN] = [1.0, [i+1, j], 0.0, False]\n",
    "                if j == 0:\n",
    "                    P[s][LEFT] = [1.0, [i, j], -1.0, False]\n",
    "                else:\n",
    "                    P[s][LEFT] = [1.0, [i, j-1], 0.0, False]\n",
    "                if j == WORLD_SIZE - 1:\n",
    "                    P[s][RIGHT] = [1.0, [i, j], -1.0, False]\n",
    "                else:\n",
    "                    P[s][RIGHT] = [1.0, [i, j+1], 0.0, False]\n",
    "                if s == A_POS:\n",
    "                    P[s][UP] = P[s][DOWN] = P[s][LEFT] = P[s][RIGHT] = [1.0, A_PRIME_POS, 10, False]\n",
    "                if s == B_POS:\n",
    "                    P[s][UP] = P[s][DOWN] = P[s][LEFT] = P[s][RIGHT] = [1.0, B_PRIME_POS, 5, False]                \n",
    "        self.P = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.array([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation \n",
    "아래 식은 policy가 주어졌을 때 value function의 값을 구하는 **Bellman equation** 이다. random policy를 집어넣은 뒤에 문제를 풀어보자.\n",
    "\n",
    "$$v_{\\pi}(s) = \\displaystyle \\sum_{a} \\pi(a \\vert s) \\sum_{s', r} p(s',r \\lvert s, a) \\big [r+\\gamma v_{\\pi}(s') \\big], \\text{ for all } s \\in S : \\text{ Bellman equation for }v_{\\pi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bellman_equation(env, policy, discount_factor=0.9, theta=1e-4):\n",
    "    V = np.zeros([WORLD_SIZE, WORLD_SIZE])\n",
    "    while True:    \n",
    "        delta = 0\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE): \n",
    "                s = (i, j)\n",
    "                v = 0\n",
    "                for a in range(env.nA):                \n",
    "                    action_prob = policy[a]\n",
    "                    prob, next_state, reward, done = env.P[s][a]\n",
    "                    v += action_prob * prob * (reward + discount_factor*V[next_state[0]][next_state[1]])\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "                V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Gridworld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a13dbe6c9237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridworld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbellman_equation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Gridworld' is not defined"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "policy = random_policy()\n",
    "value = bellman_equation(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.28592667,  8.7660323 ,  4.13828967,  4.51705437,  1.05916427],\n",
       "       [ 1.4880908 ,  2.93075511,  2.08192546,  1.60127383,  0.29403413],\n",
       "       [ 0.01982242,  0.68938662,  0.58255279,  0.22363303, -0.53666674],\n",
       "       [-0.99833352, -0.46932881, -0.40594874, -0.65335652, -1.25519416],\n",
       "       [-1.87832649, -1.37114695, -1.26420916, -1.46641581, -2.02242922]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman optimality equation\n",
    "아래 식은 value의 값을 max로 취하면서 value function의 값을 찾는 **Bellman optimality equation**이다. \n",
    "\n",
    "$$ v_{\\ast}(s) = \\max_a \\displaystyle \\sum_{s',r} p(s',r \\lvert s, a) \\big [r + \\gamma v_{\\ast}(s') \\big ] : \\text{ Bellman optimality equation for } v_{\\ast}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bellman_optimality_equation(env, discount_factor=0.9, theta=1e-4):\n",
    "    V = np.zeros([WORLD_SIZE, WORLD_SIZE])\n",
    "    while True:    \n",
    "        delta = 0\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE): \n",
    "                s = (i, j)\n",
    "                v = 0\n",
    "                v_candidate = []\n",
    "                for a in range(env.nA):          \n",
    "                    prob, next_state, reward, done = env.P[s][a]\n",
    "                    v_candidate.append(prob * (reward + discount_factor*V[next_state[0]][next_state[1]]))\n",
    "                v = max(v_candidate)\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "                V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Gridworld()\n",
    "value = bellman_optimality_equation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.9773651 ,  24.41934924,  21.97741432,  17.97741432,\n",
       "         16.17967288],\n",
       "       [ 19.77962859,  21.97741432,  19.77967288,  17.8017056 ,\n",
       "         16.02153504],\n",
       "       [ 17.80166573,  19.77967288,  17.8017056 ,  16.02153504,\n",
       "         14.41938153],\n",
       "       [ 16.02149916,  17.8017056 ,  16.02153504,  14.41938153,\n",
       "         12.97744338],\n",
       "       [ 14.41934924,  16.02153504,  14.41938153,  12.97744338,\n",
       "         11.67969904]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
